---
title: ChatGoogle
---

This library supports access to a variety of Google's models, including the Gemini 
family of models and their Nano Banana image generation model. You can access these
models through either Google's [Google AI](https://ai.google.dev/) API (sometimes also
called the Generative AI API or the AI Studio API) or through the Google Cloud Platform
[Vertex AI](https://cloud.google.com/vertex-ai) service.

This will help you getting started with `ChatGoogle` [chat models](/oss/langchain/models). 
For detailed documentation of all `ChatGoogle` features and configurations head to the 
[API reference](https://api.js.langchain.com/classes/langchain_google_vertexai.ChatVertexAI.html).

<Note>**This library is in pre-release**

  This library will be replacing the [ChatGoogleGenerativeAI](google_generative_ai)
  and [ChatVertex](google_vertex_ai) libraries.
</Note>

## Overview

### Integration details

| Class                                                                               | Package                                                                        | Serializable | PY support |                                            Downloads                                             |                                            Version                                            |
|:------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------| :---: |  :---: |:------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------:|
| [ChatGoogle](https://api.js.langchain.com/classes/langchain_google.ChatGoogle.html) | [`@langchain/google`](https://www.npmjs.com/package/@langchain/google)         | ✅ | ✅ | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google?style=flat-square&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google?style=flat-square&label=%20&) |

### Model features

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/langchain/tools) | [Structured output](/oss/langchain/structured-output) | [Image input](/oss/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/langchain/streaming/) | [Token usage](/oss/langchain/models#token-usage) | [Logprobs](/oss/langchain/models#log-probabilities) |
| :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |

Note that while logprobs are supported, Gemini has fairly restricted usage of them.

## Setup

### Credentials through AI Studio (API Key)

To use the model through Google AI Studio (sometimes called the Generative AI
API), you will need an API key. You can obtain one from the
[Google AI Studio](https://aistudio.google.com/app/apikey).

Once you have your API key, you can set it as an environment variable:

```bash
export GOOGLE_API_KEY="your-api-key"
```

Or you can pass it directly to the model constructor:

```typescript
import { ChatGoogle } from "@langchain/google";

const llm = new ChatGoogle({
  apiKey: "your-api-key",
  model: "gemini-2.5-flash",
});
```

### Credentials through Vertex AI Express Mode (API Key)

Vertex AI also supports [Express Mode](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform#express-mode), 
which allows you to use an API key for authentication. You can obtain a Vertex AI 
API key from the [Google Cloud Console](https://console.cloud.google.com/vertex-ai/studio/api-key).

Once you have your API key, you can set it as an environment variable:

```bash
export GOOGLE_API_KEY="your-api-key"
```

When using Vertex AI Express Mode, you will also need to specify the platform 
type as `gcp` when instantiating the model.

### Credentials through Vertex AI (OAuth Application Default Credentials / ADC)

For production environments on Google Cloud, it is recommended to use 
[Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/provide-credentials-adc).
This is supported in Node.js environments.

If you are running on a local machine, you can set up ADC by installing the 
[Google Cloud SDK](https://cloud.google.com/sdk) and running:

```bash
gcloud auth application-default login
```

Alternatively, you can set the `GOOGLE_APPLICATION_CREDENTIALS` environment 
variable to the path of your service account key file:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-key.json"
```

### Credentials through Vertex AI (OAuth saved credentials)

If you are running in a web environment or want to provide credentials directly,
you can use the `GOOGLE_CLOUD_CREDENTIALS` environment variable. This should 
contain the **content** of your service account key file (not the path).

```bash
export GOOGLE_CLOUD_CREDENTIALS='{"type":"service_account","project_id":"your-project-id",...}'
```

You can also provide these credentials directly in your code using the 
`credentials` parameter.

```typescript
const llm = new ChatGoogle({
  model: "gemini-2.5-flash",
  platformType: "gcp",
  credentials: {
    type: "service_account",
    project_id: "your-project-id",
    private_key_id: "your-private-key-id",
    private_key: "your-private-key",
    client_email: "your-service-account-email",
    client_id: "your-client-id",
    auth_uri: "https://accounts.google.com/o/oauth2/auth",
    token_uri: "https://oauth2.googleapis.com/token",
    auth_provider_x509_cert_url: "https://www.googleapis.com/oauth2/v1/certs",
    client_x509_cert_url: "your-cert-url",
  }
});
```

### Tracing

If you want to get automated tracing of your model calls you can also set 
your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain `ChatGoogle` integration lives in the `@langchain/google` package:

<CodeGroup>
```bash npm
npm install @langchain/google @langchain/core
```
```bash yarn
yarn add @langchain/google @langchain/core
```
```bash pnpm
pnpm add @langchain/google @langchain/core
```
</CodeGroup>

## Instantiation

The import path differs depending on whether you are running in a Node.js environment or a Web/Edge environment.

<CodeGroup>
```typescript Node.js
import { ChatGoogle } from "@langchain/google/node";
```
```typescript Web/Edge
import { ChatGoogle } from "@langchain/google";
```
</CodeGroup>

The model will automatically determine whether to use the Google AI API or Vertex AI based on your configuration:
- If you provide an `apiKey` (or set `GOOGLE_API_KEY`), it defaults to Google AI.
- If you provide `credentials` (or set `GOOGLE_APPLICATION_CREDENTIALS` / `GOOGLE_CLOUD_CREDENTIALS` in Node), it defaults to Vertex AI.

### Google AI (AI Studio)

```typescript
const llm = new ChatGoogle({
  model: "gemini-2.5-flash",
  maxRetries: 2,
  // apiKey: "...", // Optional if GOOGLE_API_KEY is set
});
```

### Vertex AI

```typescript
const llm = new ChatGoogle({
  model: "gemini-2.5-flash",
  // credentials: { ... }, // Optional if using ADC or GOOGLE_CLOUD_CREDENTIALS
});
```

### Vertex AI Express Mode

To use Vertex AI with an API key (Express Mode), you must explicitly set the `platformType`.

```typescript
const llm = new ChatGoogle({
  model: "gemini-2.5-flash",
  platformType: "gcp",
  // apiKey: "...", // Optional if GOOGLE_API_KEY is set
});
```

## Invocation

```typescript
import { HumanMessage, SystemMessage } from "@langchain/core/messages";

const aiMsg = await llm.invoke([
  new SystemMessage(
    "You are a helpful assistant that translates English to French. Translate the user sentence."
  ),
  new HumanMessage("I love programming."),
]);
```

```typescript
console.log(aiMsg.text);
```

```text
J'adore programmer.
```

## Grounding Results

Grounding allows you to provide the model with access to real-world information, 
reducing hallucinations and providing more up-to-date responses.

### Grounding with Google Search

You can use the `googleSearch` tool to ground responses with Google Search.
This is useful for questions about current events or specific facts.

<Note>
  The `googleSearchRetrieval` tool is maintained for backwards compatibility, but `googleSearch` is preferred.
</Note>

```typescript
import { ChatGoogle } from "@langchain/google";

const llm = new ChatGoogle({
  model: "gemini-2.5-flash",
}).bindTools([
  {
    googleSearch: {},
  },
]);

const res = await llm.invoke("Who won the latest World Series?");
console.log(res.text);
```

### Grounding with URL Retrieval

You can also ground responses using a specific URL.

```typescript
import { ChatGoogle } from "@langchain/google";

const llm = new ChatGoogle({
  model: "gemini-2.5-flash",
}).bindTools([
  {
    urlContext: {},
  },
]);

const prompt = "Summarize this page: https://js.langchain.com/";
const res = await llm.invoke(prompt);
console.log(res.text);
```

### Grounding with a data store

If you are using Vertex AI (`platformType: "gcp"`), you can ground responses using 
a Vertex AI Search data store.

```typescript
import { ChatGoogle } from "@langchain/google";

const projectId = "YOUR_PROJECT_ID";
const datastoreId = "YOUR_DATASTORE_ID";

const searchRetrievalToolWithDataset = {
  retrieval: {
    vertexAiSearch: {
      datastore: `projects/${projectId}/locations/global/collections/default_collection/dataStores/${datastoreId}`,
    },
    disableAttribution: false,
  },
};

const llm = new ChatGoogle({
  model: "gemini-2.5-pro",
  platformType: "gcp",
}).bindTools([searchRetrievalToolWithDataset]);

const res = await llm.invoke(
  "What is the score of Argentina vs Bolivia football game?"
);
console.log(res.text);
```

## Multimodal Requests

The `ChatGoogle` model supports multimodal requests, allowing you to send images, 
audio, and video along with text. You can use the `contentBlocks` field in your 
messages to provide these inputs in a structured way.

### Images

```typescript
import { ChatGoogle } from "@langchain/google";
import { HumanMessage } from "@langchain/core/messages";
import * as fs from "fs";

const llm = new ChatGoogle({
  model: "gemini-2.5-flash",
});

const image = fs.readFileSync("./hotdog.jpg").toString("base64");

const res = await llm.invoke([
  new HumanMessage({
    contentBlocks: [
      {
        type: "text",
        text: "What is in this image?",
      },
      {
        type: "image",
        mimeType: "image/jpeg",
        data: image,
      },
    ],
  }),
]);

console.log(res.text);
```

### Audio

```typescript
const audio = fs.readFileSync("./speech.wav").toString("base64");

const res = await llm.invoke([
  new HumanMessage({
    contentBlocks: [
      {
        type: "text",
        text: "Summarize this audio.",
      },
      {
        type: "audio",
        mimeType: "audio/wav",
        data: audio,
      },
    ],
  }),
]);

console.log(res.text);
```

### Video

```typescript
const video = fs.readFileSync("./movie.mp4").toString("base64");

const res = await llm.invoke([
  new HumanMessage({
    contentBlocks: [
      {
        type: "text",
        text: "Describe the video.",
      },
      {
        type: "video",
        mimeType: "video/mp4",
        data: video,
      },
    ],
  }),
]);

console.log(res.text);
```

## Image Generation with Nano Banana and Nano Banana Pro

To generate images, you need to use a model that supports it (such as 
`gemini-2.5-flash-image`) and configure the `responseModalities` to 
include "IMAGE".

```typescript
import { ChatGoogle } from "@langchain/google";
import * as fs from "fs";

const llm = new ChatGoogle({
  model: "gemini-2.5-flash-image",
  responseModalities: ["IMAGE", "TEXT"],
});

const res = await llm.invoke(
  "I would like to see a drawing of a house with the sun shining overhead. Drawn in crayon."
);

// Generated images are returned in the contentBlocks of the message
for (const [index, block] of res.contentBlocks.entries()) {
  if (block.type === "file" && block.data) {
    const base64Data = block.data;
    // Determine the correct file extension from the MIME type
    const mimeType = block.mimeType || "image/png";
    const extension = mimeType.split("/")[1] || "png";
    const filename = `generated_image_${index}.${extension}`;

    // Save the image to a file
    fs.writeFileSync(filename, Buffer.from(base64Data, "base64"));
    console.log(`[Saved image to ${filename}]`);
  } else if (block.type === "text") {
    console.log(block.text);
  }
}
```

## Reasoning / Thinking

Google's Gemini 2.5 and Gemini 3 models support "thinking" or "reasoning" steps. 
These models may perform reasoning even if you don't explicitly configure it, 
but the library will only return the reasoning summaries (thought blocks) if you 
explicitly set a value for how much to reason/think.

This library offers compatibility between models, allowing you to use unified parameters:

- `maxReasoningTokens` (or `thinkingBudget`): Specifies the maximum number of tokens to use for reasoning. 
  - `0`: Turns off reasoning (if supported).
  - `-1`: Uses the model's default.
  - `> 0`: Sets the specific token budget.

- `reasoningEffort` (or `thinkingLevel`): Sets the relative effort.
  - Values: `"minimal"`, `"low"`, `"medium"`, `"high"`.

```typescript
import { ChatGoogle } from "@langchain/google";

const llm = new ChatGoogle({
  model: "gemini-3-pro-preview",
  reasoningEffort: "high",
});

const res = await llm.invoke("What is the square root of 144?");

// The reasoning steps are available in the contentBlocks
const reasoningBlocks = res.contentBlocks.filter((block) => block.type === "reasoning");
reasoningBlocks.forEach((block) => {
  if (block.type === "reasoning") {
    console.log("Thought:", block.reasoning);
  }
});

console.log("Answer:", res.text);
```

<Note>
  Thought blocks also include a `reasoningContentBlock` field. This contains the `ContentBlock` based on 
  the underlying part sent by Gemini. While this is typically a text block, for multimodal models like 
  Nano Banana Pro, it could be an image or other media block.
</Note>

## API Reference

For detailed documentation of all `ChatGoogle` features and configurations head to the 
[API reference](https://api.js.langchain.com/classes/langchain_google.ChatGoogle.html).
